<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>Material Design Bootstrap</title>
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <!-- Material Design Bootstrap -->
    <link href="css/mdb.min.css" rel="stylesheet">
    <!-- Your custom styles (optional) -->
    <link href="css/style.min.css" rel="stylesheet">
</head>

<body class="grey lighten-3">

<!--Main Navigation-->
<header>

    <!-- Sidebar -->
    <div class="sidebar-fixed position-fixed">

        <a class="logo-wrapper waves-effect">
            <img src="img/lightbox/headphone-icon.png" class="img-fluid" alt="headphone-icon">
        </a>

        <div class="list-group list-group-flush">
            <a href="index.html" class="list-group-item list-group-item-action waves-effect">
                <i class="fa fa-file-text mr-3"></i>Project Statement</a>
            <a href="intro-eda.html" class="list-group-item active waves-effect">
                <i class="fa fa-table mr-3"></i>Exploratory Data Analysis</a>
            <a href="models.html" class="list-group-item list-group-item-action waves-effect">
                <i class="fa fa-pie-chart mr-3"></i>Models</a>
            <a href="results.html" class="list-group-item list-group-item-action waves-effect">
                <i class="fa fa-map mr-3"></i>Results</a>
            <a href="sources.html" class="list-group-item list-group-item-action waves-effect">
                <i class="fa fa-book mr-3"></i>Sources</a>
            <a href="team.html" class="list-group-item list-group-item-action waves-effect">
                <i class="fa fa-users mr-3"></i>The Team</a>
        </div>

    </div>
    <!-- Sidebar -->

</header>
<!--Main Navigation-->

<!--Main layout-->
<main class="pt-5 mx-lg-5">
    <div class="container-fluid">

        <!-- Heading -->
        <div class="card mb-4 wow fadeIn">

            <!--Card content-->
            <div class="card-body d-sm-flex justify-content-between">
                <h4 class="mb-2 mb-sm-0 pt-1">
                    <b>Exploratory Data Analysis</b>
                </h4>
            </div>

        </div>
        <!-- Heading -->

        <!--Grid row-->
        <div class="row wow fadeIn">

            <!--Grid column-->
            <div class="col-md-9 mb-4">

                <!--Card-->
                <div class="card">

                    <!--Card content-->
                    <div class="card-body">
                        <h4><a name="mpd"><b>Million Playlist Dataset EDA (MPD_EDA.ipynb)</b></a></h4>

                        <p>
                            For our base model we are primarily focused on the Million Playlist Dataset (MPD). Each observation corresponds to a single Spotify playlist, with various fields representing its different properties. Our first task was to determine which of these features would actually be useful for our playlist generator.
                            For example, a playlist’s name and description may provide important information about its intended theme and what the creator sees as being shared across the tracks in it.
                        </p>
                        <img src="img/graphs/top-40-names.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            The name data certainly looks promising but the same can’t be said for the descriptions.
                        </p>
                        <img src="img/graphs/top-40-names.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            While it may initially look similar to our names plot we see that the actual number of occurrences of each word in our sample is very far lower. This is because less than 2% of playlists actually have a description. With this much data missing from a predictor, it is impractical to do an imputation and benefits more from dropping the description field entirely.
                        </p>
                        <p>
                            A large number of followers may show that the user community deems a playlist to be exemplary. However, initial EDA shows that after removing some extreme outliers (potentially playlists ‘featured’ by Spotify) the mean number of followers is only about 1.5 with a standard deviation also of 1.5. If there isn’t much variation in this value among playlists it might not be useful after all.
                        </p>
                        <p>
                            Other features don’t seem to require much investigation. It’s hard to see how we benefit from knowing the number of edits a playlist has had, or if the time a playlist was last modified is really relevant.
                        </p>
                        <br>
                        <p>
                            We also looked at popularity and diversity in playlist ecosystem in terms of songs, artists, and albums.
                        </p>
                        <img src="img/graphs/popularity.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            Here we can see that a relatively small percentage of artists, songs, and albums make up a significant percentage of total playlist content, the the effect being most pronounced for songs. This information will be valuable going forward as we build our model. Knowing that songs dominate in the playlist space we may wish to take steps to prevent us from concluding that they are highly similar to many tracks when it is really their overall popularity that accounts for their ubiquity. (See final section for discussion of the Jaccard Similarity Index)
                        </p>
                        <p>
                            We also gathered some information regarding playlist composition (i.e., number of artists, tracks, and albums) and diversity (e.g., the ratio of unique artists to total artists).
                        </p>
                        <img src="img/graphs/playlist-composition.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <img src="img/graphs/playlist-diversity.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            This could prove to be important for determining how long a playlist should be if not specified by the user. Perhaps there is also a ‘sweet spot’ we should aim for as far as within-playlist diversity. It appears, for example, that most playlists do have repeated artists.
                        </p>

                    </div>

                </div>
                <!--/.Card-->

            </div>
            <!--Grid column-->
            <div class="col-md-3 mb-4">

                <!--Card-->
                <div class="card mb-4">

                    <!-- Card header -->
                    <div class="card-header text-center">
                        Navigation
                    </div>

                    <!--Card content-->
                    <div class="card-body">
                        <ul>
                            <li>
                                <a href="#mpd">Million Playlist Dataset EDA</a>
                            </li>
                            <li>
                                <a href="#spotify">Spotify API EDA</a>
                            </li>
                            <li>
                                <a href="#data-cleaning">Data Cleaning</a>
                            </li>
                        </ul>

                    </div>

                </div>
                <!--/.Card-->
            </div>

            <!--Card-->
            <div class="col-md-9 mb-4">
                <div class="card">
                    <!--Card content-->
                    <div class="card-body">
                        <h4><a name="spotify"><b>Spotify API EDA</b></a></h4>
                        <p>
                            In order to get a better idea of the composition of the playlists featured on Spotify, we pulled the tracks of top 50 playlists featured by Spotify to see if we can detect a trend of what Spotify thinks its listeners want. We then extract the audio features of each track to be able to better categorize the songs.
                        </p>
                        <table id="spotify-features-tbl" style="border: 1px solid #ddd; padding: 8px; height: 1500px; width:90%">
                            <tr style="padding-top: 12px; padding-bottom: 12px; text-align: center; background-color: #007bff;color: white;">
                                <th>KEY</th>
                                <th>VALUE TYPE</th>
                                <th>VALUE DESCRIPTION</th>
                            </tr>
                            <tr>
                                <td>acousticness</td>
                                <td>float</td>
                                <td>A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.</td>
                            </tr>
                            <tr>
                                <td>danceability</td>
                                <td>float</td>
                                <td>How suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.</td>
                            </tr>
                            <tr>
                                <td>duration_ms</td>
                                <td>int</td>
                                <td>The duration of the track in milliseconds.</td>
                            </tr>
                            <tr>
                                <td>energy</td>
                                <td>float</td>
                                <td>Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.
                                </td>
                            </tr>
                            <tr>
                                <td>instrumentalness</td>
                                <td>float</td>
                                <td>Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.
                                </td>
                            </tr>
                            <tr>
                                <td>key</td>
                                <td>int</td>
                                <td>The key the track is in. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on.</td>
                            </tr>
                            <tr>
                                <td>liveness</td>
                                <td>float</td>
                                <td>Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.</td>
                            </tr>
                            <tr>
                                <td>loudness</td>
                                <td>float</td>
                                <td>The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.</td>
                            </tr>
                            <tr>
                                <td>mode</td>
                                <td>int</td>
                                <td>Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.</td>
                            </tr>
                            <tr>
                                <td>speechiness</td>
                                <td>float</td>
                                <td>Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.
                                </td>
                            </tr>
                            <tr>
                                <td>tempo</td>
                                <td>float</td>
                                <td>The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.
                                </td>
                            </tr>
                            <tr>
                                <td>time_signature</td>
                                <td>int</td>
                                <td>An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).
                                </td>
                            </tr>
                            <tr>
                                <td>valence</td>
                                <td>float</td>
                                <td>A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).
                                </td>
                            </tr>
                        </table>
                        <p>
                            Source: Spotify API Documentation
                        </p>
                        <br>
                        <p>
                            The 825 observations were then pulled into scatter plots separated by audio feature.
                        </p>
                        <img src="img/graphs/spotify-dancebility.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            In this plot of danceability, we observe a trend of groupings of danceability with the average danceability being a little more than 5. It is likely that the groupings are from separate playlists that are either below average in danceability or playlists that are above average in danceability. The smooth transitions from low danceability to high danceability in this suggest that adjacent featured playlists in Spotify may be purposely placed near similar neighbors for a more enjoyable experience for users.
                        </p>
                        <img src="img/graphs/spotify-energy.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            Looking at the graph of the energy of each track vs the danceability of each track, we observed a similar pattern between the two graphs. According to the documentation, danceability and energy are not seemingly directed related but still seem to share an indirect relationship. Both somewhat alternate between low energy/danceability and high energy/danceability.
                        </p>
                        <img src="img/graphs/spotify-speechiness.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            Here, we observe that the mean of speechiness in the tracks were quite low (around .09). According to the documentation, a speechiness from .33-.66 most likely indicates that it is music accompanied with speech (singing is not considered speech). This indicates that a majority of the music had little to no speech in it with a majority of speechy music located together in an area. This could be reinforcing the idea that Spotify likes to have variety in their featured playlists and tend to group similar playlists today and transition between each one. Further research and more data would be needed to prove or disprove this idea.
                        </p>
                        <img src="img/graphs/spotify-liveness.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            Liveness is a measure of how loud the background noise is which is indicative of whether the tracks are recorded in a studio or performed live. The trend of this graph is extremely to the trend of the speechiness of each track. Through this, we can make better assumptions about each track. Previously, we thought that the cluster of tracks in the middle were measured high in speechiness due to being rap or another genre of talkative music. Here, we can see that some of those tracks could possibly be recorded live where there is an audience. This cross-section seem to be similar to the areas on the danceability graph where the danceability of the songs are higher.
                        </p>
                        <img src="img/graphs/spotify-feature-heatmap.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            Here, we have a visualization of the correlation matrix between each predictor that may assist in determining co-linear predictors. According to the matrix, there are a few highly negatively correlated predictors such as acousticness and energy, instrumentalness and loudness, acousticness and loudness. Predictors that are co-linear include valence and danceability, energy and danceability, energy, and valence, and acousticness and instrumentalness, as well as danceability and speechiness.
                        </p>
                    </div>
                </div>
            </div>

            <div class="col-md-9 mb-4">
                <!--Card-->
                <div class="card">
                    <!--Card content-->
                    <div class="card-body">
                        <h4><a name="data-cleaning"><b>Data Cleaning</b></a></h4>
                        <p>
                            By far the most important data for our base model are the tracks that actually constitute a given playlist. And as we’ll see, a great deal of the information the MPD provides for a playlist (e.g., num_albums, num_artists) can be pulled from the Spotify API with only the track IDs, other data (i.e., num_tracks) can simply derived from our list of tracks itself. So, for the moment, we discard everything but the ‘tracks’ field from each playlist. The tracks field is a list of dictionaries:
                        </p>
                        <img src="img/graphs/data-cleaning.png" alt="Top 40 Words Used in Playlist names"
                             class="img-fluid">
                        <p>
                            We will make the simplifying assumption that track order is not important, treating each playlist as an unordered list of tracks. We can then discard the ‘pos’ data. All other data in the dictionary could be retrieved, should we desire it, through the Spotify API with just the track_uri (i.e., track ID). A playlist for our purposes can be simplified to a list of track_ids. And so we can boil down the entire MPD to a list of lists of track ids.
                        </p>
                        <p>
                            For our first model we will actually use a string concatenation of artist name and track name as a stand-in for track ID. This will make working with the model much easier. We can simply give it input in the form of such strings (generated by the user on a whim) and get back playlists in the form of these easily interpretable strings. If we were dealing with IDs we’d have to be converting back and forth in so that we mere humans could make any sense of them.
                        </p>
                    </div>
                </div>
            </div>

            <!--Grid column-->
        </div>
    </div>
</main>
<!--Main layout-->

<!--Footer-->
<footer class="page-footer text-center font-small primary-color-dark darken-2 mt-4 wow fadeIn">

    <!--Copyright-->
    <div class="footer-copyright py-3">
        Template by
        <a href="https://mdbootstrap.com/bootstrap-tutorial/" target="_blank"> MDBootstrap.com </a>
    </div>
    <!--/.Copyright-->

</footer>
<!--/.Footer-->

<!-- SCRIPTS -->
<!-- JQuery -->
<script type="text/javascript" src="js/jquery-3.3.1.min.js"></script>
<!-- Bootstrap tooltips -->
<script type="text/javascript" src="js/popper.min.js"></script>
<!-- Bootstrap core JavaScript -->
<script type="text/javascript" src="js/bootstrap.min.js"></script>
<!-- MDB core JavaScript -->
<script type="text/javascript" src="js/mdb.min.js"></script>
<!-- Initializations -->
<script type="text/javascript">
    // Animations initialization
    new WOW().init();
</script>

</body>

</html>